{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Process of Each Functionality and Introduction of the Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code and markdowns introduce the implementation process of the 3 main functionalities of this project, including 360 object detection, 360 object tracking and 360 overtaking behaviour detection. \n",
    "\n",
    "All the functions in this notebook can be found in ./panoramic_detection/improved_OD.py, ./Object_Detection.py, ./Object_Tracking.py and ./Overtaking_Detection.py. Although some other code has been written in ./panoramic_detection/draw_output.py and ./deep_sort/, since it is low in importance and easy to understand with the comments, it is not included here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define a Function for Loading the Models (Faster RCNN and YOLO v5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Parameters of load_model():</b>\n",
    "\n",
    "- <b>model_type:</b> The name of the model to use which should be either 'YOLO' or 'Faster RCNN';\n",
    "\n",
    "- <b>input_size:</b> The maximum input size of the model, 640 by default;\n",
    "\n",
    "- <b>score_threshold:</b> The threshold of the confidence score, 0.4 by default;\n",
    "\n",
    "- <b>nms_threshold:</b> The threshold of the Non Maximum Suppression, 0.45 by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "# function used to load a YOLO or Faster RCNN model according to the users' demands\n",
    "def load_model(model_type,input_size=640,score_threshold=0.4,nms_threshold=0.45):\n",
    "\n",
    "    # first get the default config\n",
    "    cfg = get_cfg()\n",
    "\n",
    "    # choose a model from detectron2's model zoo\n",
    "    cfg.merge_from_file(\n",
    "        model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "    )\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
    "        \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
    "    )\n",
    "\n",
    "    cfg.INPUT.MAX_SIZE_TEST = input_size  # set the size of the input images\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = score_threshold  # set the threshold of the confidence score\n",
    "    cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = nms_threshold  # set the NMS threshold\n",
    "\n",
    "    # set the device to use (GPU or CPU)\n",
    "    if torch.cuda.is_available():\n",
    "        cfg.MODEL.DEVICE = \"cuda\"  \n",
    "    else:\n",
    "        cfg.MODEL.DEVICE = \"cpu\"\n",
    "\n",
    "    # only work on apple m1 mac\n",
    "    # cfg.MODEL.DEVICE = 'mps'\n",
    "\n",
    "    # create a predictor instance with the config above\n",
    "    predictor_faster_RCNN = DefaultPredictor(cfg)\n",
    "\n",
    "    # choose a model from YOLO v5 family\n",
    "    predictor_YOLO = torch.hub.load(\"ultralytics/yolov5\", \"yolov5m6\")\n",
    "    predictor_YOLO.conf = score_threshold  # set the threshold of the confidence score\n",
    "    predictor_YOLO.iou = nms_threshold  # set the NMS threshold\n",
    "    predictor_YOLO.agnostic = True  # NMS class-agnostic (i.e., only the bboxes with the same category can be eliminated after NMS)\n",
    "\n",
    "    if model_type=='Faster RCNN':\n",
    "        return predictor_faster_RCNN,cfg\n",
    "    else:\n",
    "        return predictor_YOLO,cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare Some Functions Used for the Improved Object Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Projection Transformation from Equirectangular to Perspective</b>\n",
    "\n",
    "<b>Parameters of equir2pers():</b>\n",
    "\n",
    "- <b>input_img:</b> The input image which is represented with multidimensional matrix;\n",
    "\n",
    "- <b>FOV:</b> Field of view of the sub images;\n",
    "\n",
    "- <b>THETAs:</b> A list which contains the theta of each sub image (The length should be the same as the number of sub images);\n",
    "\n",
    "- <b>PHIs:</b> A list which contains the Phi of each sub image (The length should be the same as the number of sub images);\n",
    "\n",
    "- <b>output_height, output_width:</b> Height and width of the output images (which should be the same).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Perspective_and_Equirectangular library\n",
    "import lib.Equirec2Perspec as E2P\n",
    "import lib.Perspec2Equirec as P2E\n",
    "import lib.multi_Perspec2Equirec as m_P2E\n",
    "\n",
    "# function used to split the equirectangular image into several sub images which are in perspective projection\n",
    "def equir2pers(input_img, FOV, THETAs, PHIs, output_height, output_width):\n",
    "    equ = E2P.Equirectangular(input_img)  # Load the equirectangular image\n",
    "\n",
    "    # set where to save the outputs\n",
    "    output_dir = \"./output_sub/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "\n",
    "    # maps which define the projection from equirectangular to perspective\n",
    "    lon_maps = []\n",
    "    lat_maps = []\n",
    "    imgs = []  # output images\n",
    "\n",
    "    # for each sub image\n",
    "    for i in range(len(PHIs)):\n",
    "        img1, lon_map1, lat_map1 = equ.GetPerspective(\n",
    "            FOV, THETAs[i], PHIs[i], output_height, output_width\n",
    "        )\n",
    "        # save the outputs\n",
    "        output1 = output_dir + str(i) + \".png\"\n",
    "        cv2.imwrite(output1, img1)\n",
    "        lon_maps.append(lon_map1)\n",
    "        lat_maps.append(lat_map1)\n",
    "        imgs.append(img1)\n",
    "\n",
    "    return lon_maps, lat_maps, imgs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Project the Bounding Boxes on the Sub Images Back to the Original Image and Return the Bounding Boxes whose Left/Right Borders are Tangent to a Border of the Sub Image (which are Required to be Merged)\n",
    "\n",
    "<b>Parameters of reproject_bboxes():</b>\n",
    "\n",
    "- <b>bboxes:</b> A list of bounding boxes in [x y x y] format;\n",
    "\n",
    "- <b>lon_map_original, lat_mat_original:</b> Map matrix got in the projection transformation from equirectangular to perspective (i.e., lon_maps, lat_maps returned by equir2pers());\n",
    "\n",
    "- <b>classes, scores:</b> Lists of classes and scores predicted by the object detection model;\n",
    "\n",
    "- <b>interval:</b> A value which determines how many pixels apart to calculate the corresponding coordinate point of the bounding boxes on the sub images. The smaller the interval is, the higher accuracy will be achieved;\n",
    "\n",
    "- <b>num_of_subimage:</b> Serial number of the current sub image (0 or 1 or 2 or 3), as shown in the following image;\n",
    "\n",
    "<div align=center><img src =\"./images_in_markdown/markdown1.jpg\"/></div>\n",
    "\n",
    "- <b>input_video_width, input_video_height:</b> Height and width of the input video;\n",
    "\n",
    "- <b>num_of_subimages:</b> Total number of the sub images (which should be 4 by default);\n",
    "\n",
    "- <b>threshold_of_boundary:</b> A threshold used to determine whether a left/right border of a bounding box is tangent to a border of the sub image (i.e., distance < threshold_of_boundary);\n",
    "\n",
    "- <b>is_split_image2:</b> Boolean value which used to determine whether to split the bboxes across the center line of sub image 2 into two, as shown below. \n",
    "\n",
    "<div align=center><img style=\"width:500px;\"src =\"./images_in_markdown/markdown2.png\"/></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used to reproject the bboxes on the sub images (perspective) to the original image (equirectangular)\n",
    "# and find the bboxes whose left/right border is tangent to a border of the sub image (i.e., distance < threshold_of_boundary)\n",
    "def reproject_bboxes(\n",
    "    bboxes,\n",
    "    lon_map_original,\n",
    "    lat_map_original,\n",
    "    classes,\n",
    "    scores,\n",
    "    interval,\n",
    "    num_of_subimage,\n",
    "    input_video_width,\n",
    "    input_video_height,\n",
    "    num_of_subimages,\n",
    "    threshold_of_boundary,\n",
    "    is_split_image2=True,\n",
    "):\n",
    "\n",
    "    # lists for storing the new bboxes,classes and scores after reprojection\n",
    "    new_bboxes = []\n",
    "    new_classes = []\n",
    "    new_scores = []\n",
    "\n",
    "    # variables which store the index of the bboxes (in the list new_bboxes) which coincide with the left/right boundaries of the sub image\n",
    "    left_boundary_box = None\n",
    "    right_boundary_box = None\n",
    "\n",
    "    # calculate the overlapped degree between each pair of the adjacent sub images (if the number of sub images is 4, then the result is 30)\n",
    "    overlaped_degree = (num_of_subimages * 120 - 360) / num_of_subimages\n",
    "    # calculate which subimage will be splited into two parts (if the number of sub images is 4, then the results will be image 2)\n",
    "    num_of_splited_subimage = num_of_subimages / 2\n",
    "\n",
    "    index = 0\n",
    "    # number of pixels occupied by (overlaped_degree/2) degrees on a sub image\n",
    "    margin = int(lon_map_original.shape[0] / 120 * (overlaped_degree / 2))\n",
    "\n",
    "    # for each bbox, class and score\n",
    "    for bbox, class1, score in zip(bboxes, classes, scores):\n",
    "\n",
    "        # get the coordinates of the top left point and the right bottom point\n",
    "        left_top_x = int(bbox[0])\n",
    "        left_top_y = int(bbox[1])\n",
    "        right_bottom_x = int(bbox[2])\n",
    "        right_bottom_y = int(bbox[3])\n",
    "\n",
    "        # only reproject the bboxes when they are not totally inside the overlapped area and their y-values are less than 70 degrees (or sometimes the backpack of the cyclist will be incorrectly detected as a car)\n",
    "        if (\n",
    "            margin\n",
    "            <= ((left_top_x + right_bottom_x) / 2)\n",
    "            <= (lon_map_original.shape[0] - margin)\n",
    "            and left_top_y <= lon_map_original.shape[0] / 120 * 70\n",
    "        ):\n",
    "\n",
    "            # since for an a*b sub image, the size of lon_map and lat_map is (a-1)*(b-1), when right_bottom_x or right_bottom_y equals a or b,\n",
    "            # to get the corresponding value in lon_map and lat_map (which represent the corresponding position on the original image), we have to subtract them by 1.\n",
    "            if right_bottom_x == lon_map_original.shape[0]:\n",
    "                right_bottom_x -= 1\n",
    "            if right_bottom_y == lon_map_original.shape[1]:\n",
    "                right_bottom_y -= 1\n",
    "\n",
    "            # check if a bbox coincides with the left/right boundaries of the sub image, if yes, assign its index to left_boundary_box/right_boundary_box\n",
    "            # if the bbox is large (>subimage size/5), just use the threshold to do the judgement\n",
    "            if (right_bottom_x - left_top_x) * (\n",
    "                right_bottom_y - left_top_y\n",
    "            ) < lon_map_original.shape[0] * lon_map_original.shape[0] / 5:\n",
    "                if left_top_x <= threshold_of_boundary:\n",
    "                    left_boundary_box = index\n",
    "                if right_bottom_x >= lon_map_original.shape[\n",
    "                    0\n",
    "                ] - threshold_of_boundary:\n",
    "                    right_boundary_box = index\n",
    "\n",
    "            # if the bbox is small (<=subimage size/5), set the threshold a little bit larger\n",
    "            # (based on my experience, it has better performance ^_^)\n",
    "            else:\n",
    "                if left_top_x <= (\n",
    "                    threshold_of_boundary + 15 * int(lon_map_original.shape[0] / 640)\n",
    "                ):\n",
    "                    left_boundary_box = index\n",
    "                if right_bottom_x >= lon_map_original.shape[0] - (\n",
    "                    threshold_of_boundary + 15 * int(lon_map_original.shape[0] / 640)\n",
    "                ):\n",
    "                    right_boundary_box = index\n",
    "\n",
    "            # lists used to store the corresponding x and y coordinates on the original image of each point on the bbox\n",
    "            xs = []\n",
    "            ys = []\n",
    "\n",
    "            # if the current sub image is the one which crosses the boundary (e.g., image 2 when the number of sub image is 4)\n",
    "            # and the current bbox is across the center line\n",
    "            if (\n",
    "                num_of_subimage == num_of_splited_subimage\n",
    "                and left_top_x <= int(lon_map_original.shape[0] / 2) - 1\n",
    "                and right_bottom_x >= int(lon_map_original.shape[0] / 2)\n",
    "            ):  \n",
    "                # lists used to store the x coordinates on the original image of each point on the left/right part of the bbox\n",
    "                xs_left = []\n",
    "                xs_right = []\n",
    "\n",
    "                # calculation for the left and right borders\n",
    "                for i in range(left_top_y, right_bottom_y, interval):\n",
    "                    # left border\n",
    "                    x = int(round(lon_map_original[i, left_top_x]))\n",
    "                    y = int(round(lat_map_original[i, left_top_x]))\n",
    "                    xs.append(x)\n",
    "                    ys.append(y)\n",
    "                    xs_left.append(x)\n",
    "                    # right border\n",
    "                    x = int(round(lon_map_original[i, right_bottom_x]))\n",
    "                    y = int(round(lat_map_original[i, right_bottom_x]))\n",
    "                    xs.append(x)\n",
    "                    ys.append(y)\n",
    "                    xs_right.append(x)\n",
    "\n",
    "                    \n",
    "                # calculation for the left part of the top and bottom borders\n",
    "                for i in range(\n",
    "                    left_top_x, int(lon_map_original.shape[0] / 2) - 1, interval\n",
    "                ):\n",
    "                    x = int(round(lon_map_original[left_top_y, i]))\n",
    "                    y = int(round(lat_map_original[left_top_y, i]))\n",
    "                    xs.append(x)\n",
    "                    ys.append(y)\n",
    "                    xs_left.append(x)\n",
    "                    x = int(round(lon_map_original[right_bottom_y, i]))\n",
    "                    y = int(round(lat_map_original[right_bottom_y, i]))\n",
    "                    xs.append(x)\n",
    "                    ys.append(y)\n",
    "                    xs_left.append(x)\n",
    "\n",
    "                # calculation for the right part of the top and bottom borders\n",
    "                for i in range(\n",
    "                    int(lon_map_original.shape[0] / 2), right_bottom_x, interval\n",
    "                ):\n",
    "                    x = int(round(lon_map_original[left_top_y, i]))\n",
    "                    y = int(round(lat_map_original[left_top_y, i]))\n",
    "                    xs.append(x)\n",
    "                    ys.append(y)\n",
    "                    xs_right.append(x)\n",
    "                    x = int(round(lon_map_original[right_bottom_y, i]))\n",
    "                    y = int(round(lat_map_original[right_bottom_y, i]))\n",
    "                    xs.append(x)\n",
    "                    ys.append(y)\n",
    "                    xs_right.append(x)\n",
    "\n",
    "\n",
    "                ymax = max(ys)\n",
    "                ymin = min(ys)\n",
    "                xmin_left = min(xs_left)\n",
    "                xmax_right = max(xs_right)\n",
    "\n",
    "                # if it is needed to split the bbox into two parts, create two bboxes with the MBRs of the left and right part seperately\n",
    "                if is_split_image2 == True:\n",
    "                    new_bboxes.append([xmin_left, ymin, input_video_width, ymax])\n",
    "                    new_bboxes.append([0, ymin, xmax_right, ymax])\n",
    "                    new_classes.append(int(class1))\n",
    "                    new_classes.append(int(class1))\n",
    "                    new_scores.append(score)\n",
    "                    new_scores.append(score)\n",
    "                    index += 2\n",
    "                \n",
    "                # if not, create one bbox which extends outside the right boundary\n",
    "                else:\n",
    "                    new_bboxes.append(\n",
    "                        [xmin_left, ymin, input_video_width + xmax_right, ymax]\n",
    "                    )\n",
    "                    new_classes.append(int(class1))\n",
    "                    new_scores.append(score)\n",
    "                    index += 1\n",
    "            \n",
    "            # if the current sub image is not the one which crosses the boundary\n",
    "            else:\n",
    "                # in case the interval is set larger than the length of the border, if so, set it as the length of the short side of the bbox\n",
    "                if (\n",
    "                    right_bottom_x - left_top_x < interval\n",
    "                    or right_bottom_y - left_top_y < interval\n",
    "                ):\n",
    "                    interval = min(\n",
    "                        right_bottom_x - left_top_x, right_bottom_y - left_top_y\n",
    "                    )\n",
    "                \n",
    "                # get the corresponding coordinates on the original image of each point on the boundary\n",
    "                for i in range(left_top_y, right_bottom_y, interval):\n",
    "                    x = int(round(lon_map_original[i, left_top_x]))\n",
    "                    y = int(round(lat_map_original[i, left_top_x]))\n",
    "                    xs.append(x)\n",
    "                    ys.append(y)\n",
    "                    x = int(round(lon_map_original[i, right_bottom_x]))\n",
    "                    y = int(round(lat_map_original[i, right_bottom_x]))\n",
    "                    xs.append(x)\n",
    "                    ys.append(y)\n",
    "                for i in range(left_top_x, right_bottom_x, interval):\n",
    "                    x = int(round(lon_map_original[left_top_y, i]))\n",
    "                    y = int(round(lat_map_original[left_top_y, i]))\n",
    "                    xs.append(x)\n",
    "                    ys.append(y)\n",
    "                    x = int(round(lon_map_original[right_bottom_y, i]))\n",
    "                    y = int(round(lat_map_original[right_bottom_y, i]))\n",
    "                    xs.append(x)\n",
    "                    ys.append(y)\n",
    "                \n",
    "                # create one bbox with the MBR\n",
    "                xmax = max(xs)\n",
    "                xmin = min(xs)\n",
    "                ymax = max(ys)\n",
    "                ymin = min(ys)\n",
    "                new_bboxes.append([xmin, ymin, xmax, ymax])\n",
    "                new_classes.append(int(class1))\n",
    "                new_scores.append(score)\n",
    "                index += 1\n",
    "\n",
    "    return new_bboxes, new_classes, new_scores, left_boundary_box, right_boundary_box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Match the Serial Number of the Sub Images with the Serial Number of the Boundaries\n",
    "\n",
    "As shown in the image below, according to the order of the positions in the original image, the boundaries of the sub images are labelled as boundary 1 to 8.\n",
    "\n",
    "For example, the left and right borders of image 3 are boundary 1 and boundary 4.\n",
    "\n",
    "<div align=center><img style=\"width:600px;\" src =\"./images_in_markdown/markdown3.png\"/></div>\n",
    "\n",
    "Thus, a function called number_of_left_and_right_boundary() is defined, which is used to match the serial numbers of the sub images with the serial numbers of boundaries.\n",
    "\n",
    "</br>\n",
    "\n",
    "<b>Parameters of number_of_left_and_right_boundary():</b>\n",
    "\n",
    "- <b>number_of_subimage:</b> The serial number (0,1,2,3) of a sub image.\n",
    "\n",
    "</br>\n",
    "\n",
    "<b>⚠️Attention:</b> The function is designed specifically for the case of 4 sub images, if the number of sub images is set larger, please do some corresponding modifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used to match the serial number of the sub image with the serial number of boundary\n",
    "def number_of_left_and_right_boundary(number_of_subimage):\n",
    "    if number_of_subimage == 0:\n",
    "        return [2, 5]\n",
    "    elif number_of_subimage == 1:\n",
    "        return [4, 7]\n",
    "    elif number_of_subimage == 2:\n",
    "        return [6, 1]\n",
    "    else:\n",
    "        return [0, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Merge the Bounding Boxes of the Objects which are Shown in Several Sub Images\n",
    "\n",
    "After getting the bounding boxes which are tangent to a boundary, a function called merge_bbox_across_boundary() is defined to merge them into their MBR.\n",
    "\n",
    "Bounding boxes that need to be merged can be classified into the following 2 categories:\n",
    "\n",
    "1. <b>Objects crossing 2 sub images:</b> Two bounding boxes are tangent to consecutive two boundaries (boundaries 1&2 or 3&4 or 5&6 or 7&8), as shown below:\n",
    "\n",
    "<div align=center><img style=\"width:600px;\" src =\"./images_in_markdown/markdown5.png\"/></div>\n",
    "\n",
    "2. <b>Objects crossing at least 3 sub images:</b> 4/6/8 bounding boxes are tangent to consecutive 4/6/8 boundaries. For example, for an object crossing image 0,1 and 2, as the following image shows, each of the boundary 5, 6, 7 and 8 should be tangent to a bounding box, and the bounding boxes tangent to boundary 5 and 8 should be the same.\n",
    "\n",
    "<div align=center><img style=\"width:600px;\" src =\"./images_in_markdown/markdown6.png\"/></div>\n",
    "\n",
    "<b>⚠️Attention:</b> Sometimes, the number of consecutive boundaries with a tangent bounding box can be odd (3/5/7). In such cases, just delete the first/last box and merge the remaining ones, for the one to be deleted must be included in another bounding box. For example, in the following image, each of the boundary 5, 6 and 7 is tagent to a bounding box, however, since the blue one is in the overlapped area, the contents of it are also included in the green one. So, here, we can first delete it and merge the yellow and green ones.\n",
    "\n",
    "<div align=center><img style=\"width:600px;\" src =\"./images_in_markdown/markdown4.png\"/></div>\n",
    "\n",
    "</br>\n",
    "<b>Parameters of merge_bbox_across_boundary():</b>\n",
    "\n",
    "- <b>bboxes_all:</b> List of bounding boxes after projection to the original image;\n",
    "\n",
    "- <b>classes_all, scores_all:</b> List of categories and scores of the bounding boxes;\n",
    "\n",
    "- <b>width, height:</b> Width and height of the original images;\n",
    "\n",
    "- <b>bboxes_boundary:</b> A list whose length is 8. The Nth value represents the index of the bounding box which is tangent to the Nth boundary.\n",
    "\n",
    "<b>⚠️Attention:</b> In the implementation of this function, all the possible situations are enumerated. Thus, the code can be a little bit lengthy and complex, sorry about that. In addition, the function is designed specifically for the case of 4 sub images, so if the number of sub images is set larger, please do some corresponding modifications.\n",
    "\n",
    "</br>  \n",
    "The following 3 functions are also defined in this part which are used in merge_bbox_across_boundary():\n",
    "\n",
    "- <b>weighted_average_score():</b> A function used to calculate the weighted average score of several bounding boxes;\n",
    "\n",
    "- <b>class_with_largest_score():</b> When the bboxes to merge are of different categories, use this function to choose the class with the largest weighted score as the class of the new bbox;\n",
    "\n",
    "- <b>MBR_bboxes():</b> Calculate the MBR of several bboxes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used to merge the bounding boxes of the objects which are shown in several sub images\n",
    "def merge_bbox_across_boundary(bboxes_all,classes_all,scores_all,width,height,bboxes_boundary):\n",
    "    \n",
    "    # a list to store the index of the bbox to be deleted after we merge them\n",
    "    bboxes_to_delete=[]\n",
    "\n",
    "    # first delete the bboxes which are on the boundary and are totally in the overlapped areas\n",
    "    names = locals()\n",
    "    for i in range(0,8,1):\n",
    "        if bboxes_boundary[i] !=None:\n",
    "            #  although the overlapped area is 30 degree in width, here we set the threshold as 40, for after some tests, it seems 40 can get better performance.\n",
    "            if (bboxes_all[bboxes_boundary[i]][2]-bboxes_all[bboxes_boundary[i]][0]) <= int(width/360*40):\n",
    "                bboxes_to_delete.append(bboxes_boundary[i])\n",
    "                bboxes_boundary[i] = None\n",
    "\n",
    "    # Assign each value in the array to 8 variables, just for better understanding *_*\n",
    "    bboxes_boundary1=bboxes_boundary[0]\n",
    "    bboxes_boundary2=bboxes_boundary[1]\n",
    "    bboxes_boundary3=bboxes_boundary[2]\n",
    "    bboxes_boundary4=bboxes_boundary[3]\n",
    "    bboxes_boundary5=bboxes_boundary[4]\n",
    "    bboxes_boundary6=bboxes_boundary[5]\n",
    "    bboxes_boundary7=bboxes_boundary[6]\n",
    "    bboxes_boundary8=bboxes_boundary[7] \n",
    "\n",
    "    # if the object crosses all the 4 overlapped areas (12 34 56 78)\n",
    "    if bboxes_boundary1!=None and bboxes_boundary2!=None and bboxes_boundary3!=None and bboxes_boundary4!=None and bboxes_boundary5!=None and bboxes_boundary6!=None and bboxes_boundary7!=None and bboxes_boundary8!=None and (bboxes_boundary1==bboxes_boundary4) and (bboxes_boundary3==bboxes_boundary6) and (bboxes_boundary5==bboxes_boundary8):\n",
    "            bboxes_all.extend(MBR_bboxes([bboxes_all[bboxes_boundary2],bboxes_all[bboxes_boundary1],bboxes_all[bboxes_boundary3],bboxes_all[bboxes_boundary5],bboxes_all[bboxes_boundary7]]))\n",
    "            classes_all.append(class_with_largest_score([bboxes_all[bboxes_boundary2],bboxes_all[bboxes_boundary1],bboxes_all[bboxes_boundary3],bboxes_all[bboxes_boundary5],bboxes_all[bboxes_boundary7]],[scores_all[bboxes_boundary2],scores_all[bboxes_boundary1],scores_all[bboxes_boundary3],scores_all[bboxes_boundary5],scores_all[bboxes_boundary7]],[classes_all[bboxes_boundary2],classes_all[bboxes_boundary1],classes_all[bboxes_boundary3],classes_all[bboxes_boundary5],classes_all[bboxes_boundary7]]))\n",
    "            scores_all.extend([weighted_average_score([bboxes_all[bboxes_boundary2],bboxes_all[bboxes_boundary1],bboxes_all[bboxes_boundary3],bboxes_all[bboxes_boundary5],bboxes_all[bboxes_boundary7]],[scores_all[bboxes_boundary2],scores_all[bboxes_boundary1],scores_all[bboxes_boundary3],scores_all[bboxes_boundary5],scores_all[bboxes_boundary7]])])\n",
    "            bboxes_to_delete.extend([bboxes_boundary1,bboxes_boundary2,bboxes_boundary3,bboxes_boundary4,bboxes_boundary5,bboxes_boundary6,bboxes_boundary7,bboxes_boundary8])\n",
    "    else:\n",
    "        # if the object crosses 3 overlapped areas (12 34 56)\n",
    "        if bboxes_boundary1!=None and bboxes_boundary2!=None and bboxes_boundary3!=None and bboxes_boundary4!=None and bboxes_boundary5!=None and bboxes_boundary6!=None and (bboxes_boundary1==bboxes_boundary4) and (bboxes_boundary3==bboxes_boundary6):\n",
    "                bboxes_all.extend(MBR_bboxes([bboxes_all[bboxes_boundary2],bboxes_all[bboxes_boundary1],bboxes_all[bboxes_boundary3],bboxes_all[bboxes_boundary5]]))\n",
    "                classes_all.append(class_with_largest_score([bboxes_all[bboxes_boundary2],bboxes_all[bboxes_boundary1],bboxes_all[bboxes_boundary3],bboxes_all[bboxes_boundary5]],[scores_all[bboxes_boundary2],scores_all[bboxes_boundary1],scores_all[bboxes_boundary3],scores_all[bboxes_boundary5]],[classes_all[bboxes_boundary2],classes_all[bboxes_boundary1],classes_all[bboxes_boundary3],classes_all[bboxes_boundary5]]))\n",
    "                scores_all.extend([weighted_average_score([bboxes_all[bboxes_boundary2],bboxes_all[bboxes_boundary1],bboxes_all[bboxes_boundary3],bboxes_all[bboxes_boundary5]],[scores_all[bboxes_boundary2],scores_all[bboxes_boundary1],scores_all[bboxes_boundary3],scores_all[bboxes_boundary5]])])\n",
    "                bboxes_to_delete.extend([bboxes_boundary1,bboxes_boundary2,bboxes_boundary3,bboxes_boundary4,bboxes_boundary5,bboxes_boundary6])\n",
    "\n",
    "                # if another object crosses the remaining overlapped area (78)\n",
    "                if bboxes_boundary7!=None and bboxes_boundary8!=None:\n",
    "                        bboxes_all.extend(MBR_bboxes([bboxes_all[bboxes_boundary7],bboxes_all[bboxes_boundary8]]))\n",
    "                        classes_all.append(class_with_largest_score([bboxes_all[bboxes_boundary8],bboxes_all[bboxes_boundary7]],[scores_all[bboxes_boundary8],scores_all[bboxes_boundary7]],[classes_all[bboxes_boundary8],classes_all[bboxes_boundary7]]))\n",
    "                        scores_all.extend([weighted_average_score([bboxes_all[bboxes_boundary8],bboxes_all[bboxes_boundary7]],[scores_all[bboxes_boundary8],scores_all[bboxes_boundary7]])])\n",
    "                        bboxes_to_delete.extend([bboxes_boundary7,bboxes_boundary8])\n",
    "\n",
    "        \n",
    "        # if the object crosses 3 overlapped areas (34 56 78)\n",
    "        if bboxes_boundary3!=None and bboxes_boundary4!=None and bboxes_boundary5!=None and bboxes_boundary6!=None and bboxes_boundary7!=None and bboxes_boundary8!=None and (bboxes_boundary3==bboxes_boundary6) and (bboxes_boundary5==bboxes_boundary8):\n",
    "                bboxes_all.extend(MBR_bboxes([bboxes_all[bboxes_boundary4],bboxes_all[bboxes_boundary3],bboxes_all[bboxes_boundary5],bboxes_all[bboxes_boundary7]]))\n",
    "                classes_all.append(class_with_largest_score([bboxes_all[bboxes_boundary4],bboxes_all[bboxes_boundary3],bboxes_all[bboxes_boundary5],bboxes_all[bboxes_boundary7]],[scores_all[bboxes_boundary4],scores_all[bboxes_boundary3],scores_all[bboxes_boundary5],scores_all[bboxes_boundary7]],[classes_all[bboxes_boundary4],classes_all[bboxes_boundary3],classes_all[bboxes_boundary5],classes_all[bboxes_boundary7]]))\n",
    "                scores_all.extend([weighted_average_score([bboxes_all[bboxes_boundary4],bboxes_all[bboxes_boundary3],bboxes_all[bboxes_boundary5],bboxes_all[bboxes_boundary7]],[scores_all[bboxes_boundary4],scores_all[bboxes_boundary3],scores_all[bboxes_boundary5],scores_all[bboxes_boundary7]])])\n",
    "                bboxes_to_delete.extend([bboxes_boundary3,bboxes_boundary4,bboxes_boundary5,bboxes_boundary6,bboxes_boundary7,bboxes_boundary8])\n",
    "\n",
    "                # if another object crosses the remaining overlapped area (12)\n",
    "                if bboxes_boundary1!=None and bboxes_boundary2!=None:\n",
    "                        bboxes_all.extend(MBR_bboxes([bboxes_all[bboxes_boundary2],bboxes_all[bboxes_boundary1]]))\n",
    "                        classes_all.append(class_with_largest_score([bboxes_all[bboxes_boundary2],bboxes_all[bboxes_boundary1]],[scores_all[bboxes_boundary2],scores_all[bboxes_boundary1]],[classes_all[bboxes_boundary2],classes_all[bboxes_boundary1]]))\n",
    "                        scores_all.extend([weighted_average_score([bboxes_all[bboxes_boundary2],bboxes_all[bboxes_boundary1]],[scores_all[bboxes_boundary2],scores_all[bboxes_boundary1]])])\n",
    "                        bboxes_to_delete.extend([bboxes_boundary1,bboxes_boundary2])\n",
    "\n",
    "        else:\n",
    "            # if the object crosses 2 overlapped areas (12 34)\n",
    "            if bboxes_boundary1!=None and bboxes_boundary2!=None and bboxes_boundary3!=None and bboxes_boundary4!=None and (bboxes_boundary1==bboxes_boundary4):\n",
    "                    bboxes_all.extend(MBR_bboxes([bboxes_all[bboxes_boundary2],bboxes_all[bboxes_boundary1],bboxes_all[bboxes_boundary3]]))\n",
    "                    classes_all.append(class_with_largest_score([bboxes_all[bboxes_boundary2],bboxes_all[bboxes_boundary1],bboxes_all[bboxes_boundary3]],[scores_all[bboxes_boundary2],scores_all[bboxes_boundary1],scores_all[bboxes_boundary3]],[classes_all[bboxes_boundary2],classes_all[bboxes_boundary1],classes_all[bboxes_boundary3]]))\n",
    "                    scores_all.extend([weighted_average_score([bboxes_all[bboxes_boundary2],bboxes_all[bboxes_boundary1],bboxes_all[bboxes_boundary3]],[scores_all[bboxes_boundary2],scores_all[bboxes_boundary1],scores_all[bboxes_boundary3]])])\n",
    "                    bboxes_to_delete.extend([bboxes_boundary1,bboxes_boundary2,bboxes_boundary3,bboxes_boundary4])\n",
    "\n",
    "                    # if another object crosses the remaining overlapped area (56)\n",
    "                    if bboxes_boundary5!=None and bboxes_boundary6!=None:\n",
    "                            bboxes_all.extend(MBR_bboxes([bboxes_all[bboxes_boundary5],bboxes_all[bboxes_boundary6]]))\n",
    "                            classes_all.append(class_with_largest_score([bboxes_all[bboxes_boundary6],bboxes_all[bboxes_boundary5]],[scores_all[bboxes_boundary6],scores_all[bboxes_boundary5]],[classes_all[bboxes_boundary6],classes_all[bboxes_boundary5]]))\n",
    "                            scores_all.extend([weighted_average_score([bboxes_all[bboxes_boundary6],bboxes_all[bboxes_boundary5]],[scores_all[bboxes_boundary6],scores_all[bboxes_boundary5]])])\n",
    "                            bboxes_to_delete.extend([bboxes_boundary5,bboxes_boundary6])\n",
    "\n",
    "                    # if another object crosses the remaining overlapped area (78)\n",
    "                    if bboxes_boundary7!=None and bboxes_boundary8!=None:\n",
    "                            bboxes_all.extend(MBR_bboxes([bboxes_all[bboxes_boundary7],bboxes_all[bboxes_boundary8]]))\n",
    "                            classes_all.append(class_with_largest_score([bboxes_all[bboxes_boundary8],bboxes_all[bboxes_boundary7]],[scores_all[bboxes_boundary8],scores_all[bboxes_boundary7]],[classes_all[bboxes_boundary8],classes_all[bboxes_boundary7]]))\n",
    "                            scores_all.extend([weighted_average_score([bboxes_all[bboxes_boundary8],bboxes_all[bboxes_boundary7]],[scores_all[bboxes_boundary8],scores_all[bboxes_boundary7]])])\n",
    "                            bboxes_to_delete.extend([bboxes_boundary7,bboxes_boundary8])\n",
    "\n",
    "            # if the object crosses 2 overlapped areas (34 56)\n",
    "            if bboxes_boundary3!=None and bboxes_boundary4!=None and bboxes_boundary5!=None and bboxes_boundary6!=None and (bboxes_boundary3==bboxes_boundary6):\n",
    "                    bboxes_all.extend(MBR_bboxes([bboxes_all[bboxes_boundary4],bboxes_all[bboxes_boundary3],bboxes_all[bboxes_boundary5]]))\n",
    "                    classes_all.append(class_with_largest_score([bboxes_all[bboxes_boundary4],bboxes_all[bboxes_boundary3],bboxes_all[bboxes_boundary5]],[scores_all[bboxes_boundary4],scores_all[bboxes_boundary3],scores_all[bboxes_boundary5]],[classes_all[bboxes_boundary4],classes_all[bboxes_boundary3],classes_all[bboxes_boundary5]]))\n",
    "                    scores_all.extend([weighted_average_score([bboxes_all[bboxes_boundary4],bboxes_all[bboxes_boundary3],bboxes_all[bboxes_boundary5]],[scores_all[bboxes_boundary4],scores_all[bboxes_boundary3],scores_all[bboxes_boundary5]])])\n",
    "                    bboxes_to_delete.extend([bboxes_boundary3,bboxes_boundary4,bboxes_boundary5,bboxes_boundary6])\n",
    "\n",
    "                    # if another object crosses the remaining overlapped area (12)\n",
    "                    if bboxes_boundary1!=None and bboxes_boundary2!=None:\n",
    "                            bboxes_all.extend(MBR_bboxes([bboxes_all[bboxes_boundary2],bboxes_all[bboxes_boundary1]]))\n",
    "                            classes_all.append(class_with_largest_score([bboxes_all[bboxes_boundary2],bboxes_all[bboxes_boundary1]],[scores_all[bboxes_boundary2],scores_all[bboxes_boundary1]],[classes_all[bboxes_boundary2],classes_all[bboxes_boundary1]]))\n",
    "                            scores_all.extend([weighted_average_score([bboxes_all[bboxes_boundary2],bboxes_all[bboxes_boundary1]],[scores_all[bboxes_boundary2],scores_all[bboxes_boundary1]])])\n",
    "                            bboxes_to_delete.extend([bboxes_boundary1,bboxes_boundary2])\n",
    "\n",
    "                    # if another object crosses the remaining overlapped area (78)\n",
    "                    if bboxes_boundary7!=None and bboxes_boundary8!=None:\n",
    "                            bboxes_all.extend(MBR_bboxes([bboxes_all[bboxes_boundary7],bboxes_all[bboxes_boundary8]]))\n",
    "                            classes_all.append(class_with_largest_score([bboxes_all[bboxes_boundary8],bboxes_all[bboxes_boundary7]],[scores_all[bboxes_boundary8],scores_all[bboxes_boundary7]],[classes_all[bboxes_boundary8],classes_all[bboxes_boundary7]]))\n",
    "                            scores_all.extend([weighted_average_score([bboxes_all[bboxes_boundary8],bboxes_all[bboxes_boundary7]],[scores_all[bboxes_boundary8],scores_all[bboxes_boundary7]])])\n",
    "                            bboxes_to_delete.extend([bboxes_boundary7,bboxes_boundary8])\n",
    "\n",
    "\n",
    "            # if the object crosses 2 overlapped areas (56 78)\n",
    "            if bboxes_boundary5!=None and bboxes_boundary6!=None and bboxes_boundary7!=None and bboxes_boundary8!=None and (bboxes_boundary5==bboxes_boundary8):\n",
    "                    bboxes_all.extend(MBR_bboxes([bboxes_all[bboxes_boundary6],bboxes_all[bboxes_boundary5],bboxes_all[bboxes_boundary7]]))\n",
    "                    classes_all.append(class_with_largest_score([bboxes_all[bboxes_boundary6],bboxes_all[bboxes_boundary5],bboxes_all[bboxes_boundary7]],[scores_all[bboxes_boundary6],scores_all[bboxes_boundary5],scores_all[bboxes_boundary7]],[classes_all[bboxes_boundary6],classes_all[bboxes_boundary5],classes_all[bboxes_boundary7]]))\n",
    "                    scores_all.extend([weighted_average_score([bboxes_all[bboxes_boundary6],bboxes_all[bboxes_boundary5],bboxes_all[bboxes_boundary7]],[scores_all[bboxes_boundary6],scores_all[bboxes_boundary5],scores_all[bboxes_boundary7]])])\n",
    "                    bboxes_to_delete.extend([bboxes_boundary5,bboxes_boundary6,bboxes_boundary7,bboxes_boundary8])\n",
    "\n",
    "                    # if another object crosses the remaining overlapped area (12)\n",
    "                    if bboxes_boundary1!=None and bboxes_boundary2!=None:\n",
    "                            bboxes_all.extend(MBR_bboxes([bboxes_all[bboxes_boundary2],bboxes_all[bboxes_boundary1]]))\n",
    "                            classes_all.append(class_with_largest_score([bboxes_all[bboxes_boundary2],bboxes_all[bboxes_boundary1]],[scores_all[bboxes_boundary2],scores_all[bboxes_boundary1]],[classes_all[bboxes_boundary2],classes_all[bboxes_boundary1]]))\n",
    "                            scores_all.extend([weighted_average_score([bboxes_all[bboxes_boundary2],bboxes_all[bboxes_boundary1]],[scores_all[bboxes_boundary2],scores_all[bboxes_boundary1]])])\n",
    "                            bboxes_to_delete.extend([bboxes_boundary1,bboxes_boundary2])\n",
    "\n",
    "                    # if another object crosses the remaining overlapped area (34)\n",
    "                    if bboxes_boundary3!=None and bboxes_boundary4!=None:\n",
    "                            bboxes_all.extend(MBR_bboxes([bboxes_all[bboxes_boundary3],bboxes_all[bboxes_boundary4]]))\n",
    "                            classes_all.append(class_with_largest_score([bboxes_all[bboxes_boundary4],bboxes_all[bboxes_boundary3]],[scores_all[bboxes_boundary4],scores_all[bboxes_boundary3]],[classes_all[bboxes_boundary4],classes_all[bboxes_boundary3]]))\n",
    "                            scores_all.extend([weighted_average_score([bboxes_all[bboxes_boundary4],bboxes_all[bboxes_boundary3]],[scores_all[bboxes_boundary4],scores_all[bboxes_boundary3]])])\n",
    "                            bboxes_to_delete.extend([bboxes_boundary3,bboxes_boundary4])\n",
    "                            \n",
    "            else:\n",
    "                # if the object crosses 1 overlapped area (12)\n",
    "                if bboxes_boundary1!=None and bboxes_boundary2!=None:\n",
    "                        bboxes_all.extend(MBR_bboxes([bboxes_all[bboxes_boundary2],bboxes_all[bboxes_boundary1]]))\n",
    "                        classes_all.append(class_with_largest_score([bboxes_all[bboxes_boundary2],bboxes_all[bboxes_boundary1]],[scores_all[bboxes_boundary2],scores_all[bboxes_boundary1]],[classes_all[bboxes_boundary2],classes_all[bboxes_boundary1]]))\n",
    "                        scores_all.extend([weighted_average_score([bboxes_all[bboxes_boundary2],bboxes_all[bboxes_boundary1]],[scores_all[bboxes_boundary2],scores_all[bboxes_boundary1]])])\n",
    "                        bboxes_to_delete.extend([bboxes_boundary1,bboxes_boundary2])\n",
    "\n",
    "                # if the object crosses 1 overlapped area (34)\n",
    "                if bboxes_boundary3!=None and bboxes_boundary4!=None:\n",
    "                        bboxes_all.extend(MBR_bboxes([bboxes_all[bboxes_boundary3],bboxes_all[bboxes_boundary4]]))\n",
    "                        classes_all.append(class_with_largest_score([bboxes_all[bboxes_boundary4],bboxes_all[bboxes_boundary3]],[scores_all[bboxes_boundary4],scores_all[bboxes_boundary3]],[classes_all[bboxes_boundary4],classes_all[bboxes_boundary3]]))\n",
    "                        scores_all.extend([weighted_average_score([bboxes_all[bboxes_boundary4],bboxes_all[bboxes_boundary3]],[scores_all[bboxes_boundary4],scores_all[bboxes_boundary3]])])\n",
    "                        bboxes_to_delete.extend([bboxes_boundary3,bboxes_boundary4])\n",
    "\n",
    "                # if the object crosses 1 overlapped area (56)\n",
    "                if bboxes_boundary5!=None and bboxes_boundary6!=None:\n",
    "                        bboxes_all.extend(MBR_bboxes([bboxes_all[bboxes_boundary5],bboxes_all[bboxes_boundary6]]))\n",
    "                        classes_all.append(class_with_largest_score([bboxes_all[bboxes_boundary6],bboxes_all[bboxes_boundary5]],[scores_all[bboxes_boundary6],scores_all[bboxes_boundary5]],[classes_all[bboxes_boundary6],classes_all[bboxes_boundary5]]))\n",
    "                        scores_all.extend([weighted_average_score([bboxes_all[bboxes_boundary6],bboxes_all[bboxes_boundary5]],[scores_all[bboxes_boundary6],scores_all[bboxes_boundary5]])])\n",
    "                        bboxes_to_delete.extend([bboxes_boundary5,bboxes_boundary6])\n",
    "\n",
    "                # if the object crosses 1 overlapped area (78)\n",
    "                if bboxes_boundary7!=None and bboxes_boundary8!=None:\n",
    "                        bboxes_all.extend(MBR_bboxes([bboxes_all[bboxes_boundary7],bboxes_all[bboxes_boundary8]]))\n",
    "                        classes_all.append(class_with_largest_score([bboxes_all[bboxes_boundary8],bboxes_all[bboxes_boundary7]],[scores_all[bboxes_boundary8],scores_all[bboxes_boundary7]],[classes_all[bboxes_boundary8],classes_all[bboxes_boundary7]]))\n",
    "                        scores_all.extend([weighted_average_score([bboxes_all[bboxes_boundary8],bboxes_all[bboxes_boundary7]],[scores_all[bboxes_boundary8],scores_all[bboxes_boundary7]])])\n",
    "                        bboxes_to_delete.extend([bboxes_boundary7,bboxes_boundary8])\n",
    "\n",
    "    # delete the boxes that have been merged from the lists\n",
    "    bboxes_to_delete=list(set(bboxes_to_delete))\n",
    "    bboxes_to_delete.sort(reverse=True)\n",
    "    for i in bboxes_to_delete:\n",
    "        bboxes_all.pop(i)\n",
    "        classes_all.pop(i)\n",
    "        scores_all.pop(i)\n",
    "\n",
    "    return bboxes_all, classes_all, scores_all\n",
    "    \n",
    "\n",
    "# function used to calculate the weighted average score of several bboxes\n",
    "def weighted_average_score(bboxes,scores):\n",
    "    sum=0\n",
    "    sum_area=0\n",
    "    for bbox,score in zip(bboxes,scores):\n",
    "        area=(bbox[3]-bbox[1])*(bbox[2]-bbox[0])\n",
    "        sum+=score*area\n",
    "        sum_area+=area\n",
    "    return np.float32(sum/sum_area)\n",
    "\n",
    "# function used to choose the class with the largest weighted score as the class of the new merged bbox\n",
    "def class_with_largest_score(bboxes,scores,classes):\n",
    "    sum_area=0\n",
    "    score_multi_area=[]\n",
    "    for bbox,score in zip(bboxes,scores):\n",
    "        area=(bbox[3]-bbox[1])*(bbox[2]-bbox[0])\n",
    "        score_multi_area.append(area*score)\n",
    "        sum_area+=area\n",
    "    weighted_score = [i / sum_area for i in score_multi_area]\n",
    "    return classes[weighted_score.index(max(weighted_score))]\n",
    "\n",
    "# function used to calculate the MBR of several connected bboxes\n",
    "def MBR_bboxes(bboxes):\n",
    "    xs=[]\n",
    "    ys=[]\n",
    "    for bbox in bboxes:\n",
    "        xs.append(bbox[0])\n",
    "        xs.append(bbox[2])\n",
    "        ys.append(bbox[1])\n",
    "        ys.append(bbox[3])\n",
    "    return [[min(xs),min(ys),max(xs),max(ys)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. Other Functions Used for Improving Object Detection </b>\n",
    "\n",
    "- <b>filter_classes():</b> The pre-trained YOLO and Faster RCNN output the detection results of all the categories in COCO, but we only need part of them. This function is used to filter out the bounding boxes of the categories we need;\n",
    "\n",
    "- <b>project_class():</b> The output classes are in the form like [0,1,2,3,5,7,9], but in the annotated dataset, the objects' classes are labeled as [0,1,2,3,4,5,6], so this function is used to project the class_id when doing evaluations;\n",
    "\n",
    "  <b>⚠️Attention:</b> If the categories to detect are changed, values in this function should also be changed.\n",
    "\n",
    "- <b>xyxy2xcycwh():</b> A function used to transform the output from [x1,y1,x2,y2] format to [x_centre, y_centre, width, height]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used to filter the bboxes according to the classes we need\n",
    "def filter_classes(bboxes_all, classes_all, scores_all, class_needed):\n",
    "    bboxes_all = bboxes_all.tolist()\n",
    "    classes_all = classes_all.tolist()\n",
    "    scores_all = scores_all.tolist()\n",
    "    # remove the bboxes which are not belong to the needed classes from the lists\n",
    "    for i in range(len(classes_all), 0, -1):\n",
    "        if classes_all[i - 1] not in class_needed:\n",
    "            bboxes_all.pop(i - 1)\n",
    "            classes_all.pop(i - 1)\n",
    "            scores_all.pop(i - 1)\n",
    "    return bboxes_all, classes_all, scores_all\n",
    "\n",
    "# function used to project the class id from [0,1,2,3,5,7,9] to [0,6] to match our annotations\n",
    "def project_class(classes):\n",
    "    for index,class1 in enumerate(classes):\n",
    "        if class1==5:\n",
    "            classes[index]=4\n",
    "        elif class1==7:\n",
    "            classes[index]=5\n",
    "        elif class1==9:\n",
    "            classes[index]=6\n",
    "    return classes\n",
    "\n",
    "# A function used to transform the output from [x1,y1,x2,y2] format to [x_centre, y_centre, width, height].\n",
    "def xyxy2xcycwh(bboxes):\n",
    "    bboxes_new = []\n",
    "    for bbox in bboxes:\n",
    "        bboxes_new.append(\n",
    "            [\n",
    "                (bbox[0] + bbox[2]) / 2,\n",
    "                (bbox[1] + bbox[3]) / 2,\n",
    "                (bbox[2] - bbox[0]),\n",
    "                (bbox[3] - bbox[1]),\n",
    "            ]\n",
    "        )\n",
    "    return bboxes_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define the Process of the Improved Object Detection on One Frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using the functions above, the process of the improved object detection on one image frame is defined as a function called predict_one_frame().\n",
    "\n",
    "<b>Parameters of predict_one_frame():</b>\n",
    "\n",
    "- <b>FOV:</b> Field of view of the sub images;\n",
    "\n",
    "- <b>THETAs:</b> A list which contains the theta of each sub image (The length should be the same as the number of sub images);\n",
    "\n",
    "- <b>PHIs:</b> A list which contains the Phi of each sub image (The length should be the same as the number of sub images);\n",
    "\n",
    "- <b>im:</b> The image on which to do the object detection;\n",
    "\n",
    "- <b>predictor:</b> A YOLO v5 or Faster RCNN object detection model;\n",
    "\n",
    "- <b>video_width, video_height:</b> Height and width of the input image frame;\n",
    "\n",
    "- <b>sub_image_width:</b> Width (or height) of the sub images;\n",
    "\n",
    "- <b>classes_to_detect:</b> Index numbers in COCO of the classes we need to detect, [0, 1, 2, 3, 5, 7, 9] by default;\n",
    "\n",
    "- <b>is_project_class:</b> A boolean value which determines whether to project the original class_ids of the outputs according to our annotations using project_class();\n",
    "\n",
    "- <b>use_mymodel:</b> A boolean value which determines whether to use the improved object detection model, if False, instead of being split into 4 parts, the image will be detected as a whole;\n",
    "\n",
    "- <b>model:</b> Name of the model to use, which should be either \"Faster RCNN\" or \"YOLO\";\n",
    "\n",
    "- <b>is_split_image2:</b> A boolean value which determines whether to split the bboxes across the center line of sub image 2 into two when reprojecting the bboxes back to the original image using reproject_bboxes().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from detectron2.structures.instances import Instances\n",
    "from detectron2.structures.boxes import Boxes\n",
    "from detectron2.layers import batched_nms\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "# function used to do object detection on one image frame\n",
    "def predict_one_frame(\n",
    "    FOV,\n",
    "    THETAs,\n",
    "    PHIs,\n",
    "    im,\n",
    "    predictor,\n",
    "    video_width,\n",
    "    video_height,\n",
    "    sub_image_width,\n",
    "    classes_to_detect=[0, 1, 2, 3, 5, 7, 9],\n",
    "    is_project_class=False,\n",
    "    use_mymodel=True,\n",
    "    model=\"Faster RCNN\",\n",
    "    split_image2=True,\n",
    "):\n",
    "\n",
    "    # for checking the processing speed, record the current time first\n",
    "    time1 = time.time()\n",
    "    \n",
    "    # if the user chooses to use the improved object detection model\n",
    "    if use_mymodel:\n",
    "        # split the frame into 4 sub images (of perspective projection) and get the maps and the output images\n",
    "        lon_maps, lat_maps, subimgs = equir2pers(\n",
    "            im, FOV, THETAs, PHIs, sub_image_width, sub_image_width\n",
    "        )\n",
    "\n",
    "        # lists for storing the detection results from all the sub images\n",
    "        bboxes_all = []\n",
    "        classes_all = []\n",
    "        scores_all = []\n",
    "\n",
    "        # list for storing the index of the bounding boxes which intersect with the boundaries of the sub images\n",
    "        bboxes_boundary = [None] * 8\n",
    "\n",
    "        # if a Faster RCNN model is being used\n",
    "        if model == \"Faster RCNN\":\n",
    "            # for each sub image\n",
    "            for i in range(len(subimgs)):\n",
    "                # get the detection results with the predictor\n",
    "                outputs1 = predictor(subimgs[i])\n",
    "\n",
    "                # --------  if you want to save and check the detail of the results on each sub image, run the code below  ----------\n",
    "                # v1 = Visualizer(\n",
    "                #     subimgs[i][:, :, ::-1],\n",
    "                #     MetadataCatalog.get(cfg.DATASETS.TRAIN[0]),\n",
    "                #     scale=1.0,\n",
    "                # )\n",
    "                # im1 = v1.draw_instance_predictions(outputs1[\"instances\"].to(\"cpu\"))\n",
    "                # cv2.imwrite(\n",
    "                #     \"./outtest/subdetect\" + str(i) + \".png\", im1.get_image()[:, :, ::-1]\n",
    "                # )\n",
    "                # --------  end of this part  ----------\n",
    "\n",
    "                # get the bboxes, classes and scores of the instances detected\n",
    "                bboxes = outputs1[\"instances\"].pred_boxes.tensor.cpu().numpy()\n",
    "                classes = outputs1[\"instances\"].pred_classes.cpu().numpy()\n",
    "                scores = outputs1[\"instances\"].scores.cpu().numpy()\n",
    "\n",
    "                # do NMS on the bboxes despite the category\n",
    "                # keep_boxes is a list which stores the index of the bboxes to keep after NMS\n",
    "                keep_boxes = torchvision.ops.nms(\n",
    "                    torch.tensor(bboxes), torch.tensor(scores), 0.45\n",
    "                )\n",
    "\n",
    "                # for each bbox in the current sub image, reproject it to the original image\n",
    "                (\n",
    "                    reprojected_bboxes,\n",
    "                    classes,\n",
    "                    scores,\n",
    "                    left_boundary_box,\n",
    "                    right_boundary_box,\n",
    "                ) = reproject_bboxes(\n",
    "                    torch.tensor(bboxes)[keep_boxes],\n",
    "                    lon_maps[i],\n",
    "                    lat_maps[i],\n",
    "                    torch.tensor(classes)[keep_boxes],\n",
    "                    torch.tensor(scores)[keep_boxes],\n",
    "                    10,\n",
    "                    i,\n",
    "                    video_width,\n",
    "                    video_height,\n",
    "                    len(subimgs),\n",
    "                    sub_image_width / 640 * 20,\n",
    "                    split_image2,\n",
    "                )\n",
    "\n",
    "                # get the index of the bboxes which intersect the boundaries of the sub images\n",
    "                if left_boundary_box != None:\n",
    "                    bboxes_boundary[\n",
    "                        number_of_left_and_right_boundary(i)[0]\n",
    "                    ] = left_boundary_box + len(bboxes_all)\n",
    "                if right_boundary_box != None:\n",
    "                    bboxes_boundary[\n",
    "                        number_of_left_and_right_boundary(i)[1]\n",
    "                    ] = right_boundary_box + len(bboxes_all)\n",
    "\n",
    "                # add the bboxes after reprojection to the lists which contain bboxes from all the sub images\n",
    "                bboxes_all = bboxes_all + reprojected_bboxes\n",
    "                classes_all = classes_all + classes\n",
    "                scores_all = scores_all + scores\n",
    "\n",
    "        # if a YOLO model is being used\n",
    "        elif model == \"YOLO\":\n",
    "\n",
    "            # for each sub image, first change the color from BGR to RGB\n",
    "            for i in range(len(subimgs)):\n",
    "                subimgs[i] = cv2.cvtColor(subimgs[i], cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # YOLO supports detecting several images at the same time, so input all the sub images at once to the predictor\n",
    "            results = predictor(subimgs, size=sub_image_width)  # includes NMS\n",
    "            \n",
    "            # --------  if you want to save and check the detail of the results on each sub image, run the code below  ----------\n",
    "            # results.save()\n",
    "            # --------  end of this part  ----------\n",
    "            \n",
    "            # for each sub image\n",
    "            for i in range(len(subimgs)):\n",
    "                # Originally, YOLO outputs the positions using the relative coordinates [0-1], so transform the output format by multiplying by the width/height of the sub image\n",
    "                bboxes = (\n",
    "                    results.xyxyn[i].cpu().numpy()[:, 0:4]\n",
    "                    * [\n",
    "                        sub_image_width,\n",
    "                        sub_image_width,\n",
    "                        sub_image_width,\n",
    "                        sub_image_width,\n",
    "                    ]\n",
    "                ).tolist()\n",
    "                classes = list(map(int, results.xyxyn[i].cpu().numpy()[:, 5].tolist()))\n",
    "                scores = results.xyxyn[i].cpu().numpy()[:, 4].tolist()\n",
    "\n",
    "                # for each bbox in the current sub image, reproject it to the original image\n",
    "                (\n",
    "                    reprojected_bboxes,\n",
    "                    classes,\n",
    "                    scores,\n",
    "                    left_boundary_box,\n",
    "                    right_boundary_box,\n",
    "                ) = reproject_bboxes(\n",
    "                    bboxes,\n",
    "                    lon_maps[i],\n",
    "                    lat_maps[i],\n",
    "                    classes,\n",
    "                    scores,\n",
    "                    10,\n",
    "                    i,\n",
    "                    video_width,\n",
    "                    video_height,\n",
    "                    len(subimgs),\n",
    "                    sub_image_width / 640 * 20,\n",
    "                    split_image2,\n",
    "                )\n",
    "\n",
    "                # get the index of the bboxes which intersect the boundaries of the sub images\n",
    "                if left_boundary_box != None:\n",
    "                    bboxes_boundary[\n",
    "                        number_of_left_and_right_boundary(i)[0]\n",
    "                    ] = left_boundary_box + len(bboxes_all)\n",
    "                if right_boundary_box != None:\n",
    "                    bboxes_boundary[\n",
    "                        number_of_left_and_right_boundary(i)[1]\n",
    "                    ] = right_boundary_box + len(bboxes_all)\n",
    "\n",
    "                # add the bboxes after reprojection to the lists which contain bboxes from all the sub images\n",
    "                bboxes_all = bboxes_all + reprojected_bboxes\n",
    "                classes_all = classes_all + classes\n",
    "                scores_all = scores_all + scores\n",
    "\n",
    "        # merge the boxes which goes across the boundaries with merge_bbox_across_boundary()\n",
    "        bboxes_all, classes_all, scores_all = merge_bbox_across_boundary(\n",
    "            bboxes_all,\n",
    "            classes_all,\n",
    "            scores_all,\n",
    "            video_width,\n",
    "            video_height,\n",
    "            bboxes_boundary,\n",
    "        )\n",
    "\n",
    "        # do NMS on the output bboxes again to get the index of the boxes which should be kept\n",
    "        keep = batched_nms(\n",
    "            torch.tensor(bboxes_all),\n",
    "            torch.tensor(scores_all),\n",
    "            torch.tensor(classes_all),\n",
    "            0.3,\n",
    "        )\n",
    "\n",
    "        # only keep the instances of the classes we need (person, bike, car, motorbike, bus, truck, traffic light by default)\n",
    "        bboxes_all, classes_all, scores_all = filter_classes(\n",
    "            torch.tensor(bboxes_all)[keep],\n",
    "            torch.tensor(classes_all)[keep],\n",
    "            torch.tensor(scores_all)[keep],\n",
    "            classes_to_detect,\n",
    "        )\n",
    "\n",
    "        # if needed, project the class into [0,6] (to match with the annotations in our dataset)\n",
    "        if is_project_class == True:\n",
    "            classes_all = project_class(classes_all)\n",
    "\n",
    "    # if the user chooses to use the original object detection model\n",
    "    else:\n",
    "        # if a Faster RCNN model is being used\n",
    "        if model == \"Faster RCNN\":\n",
    "            # get the outputs and do NMS on them\n",
    "            outputs1 = predictor(im)\n",
    "            bboxes_all = outputs1[\"instances\"].pred_boxes.tensor.cpu().numpy()\n",
    "            classes_all = outputs1[\"instances\"].pred_classes.cpu().numpy()\n",
    "            scores_all = outputs1[\"instances\"].scores.cpu().numpy()\n",
    "            keep_boxes = torchvision.ops.nms(\n",
    "                torch.tensor(bboxes_all), torch.tensor(scores_all), 0.45\n",
    "            )\n",
    "            bboxes_all = (\n",
    "                outputs1[\"instances\"].pred_boxes.tensor.cpu().numpy()[keep_boxes]\n",
    "            )\n",
    "            classes_all = outputs1[\"instances\"].pred_classes.cpu().numpy()[keep_boxes]\n",
    "            scores_all = outputs1[\"instances\"].scores.cpu().numpy()[keep_boxes]\n",
    "        \n",
    "        # if a YOLO model is being used\n",
    "        elif model == \"YOLO\":\n",
    "            # change the color from BGR to RGB\n",
    "            im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "            # get the outputs\n",
    "            results = predictor(im, size=sub_image_width)  # NMS included\n",
    "            bboxes_all = (\n",
    "                results.xyxyn[0].cpu().numpy()[:, 0:4]\n",
    "                * [video_width, video_height, video_width, video_height]\n",
    "            ).tolist()\n",
    "            classes_all = list(map(int, results.xyxyn[0].cpu().numpy()[:, 5].tolist()))\n",
    "            scores_all = results.xyxyn[0].cpu().numpy()[:, 4].tolist()\n",
    "\n",
    "        # only keep the instances of the classes we need (person, bike, car, motorbike, bus, truck, traffic light)\n",
    "        bboxes_all, classes_all, scores_all = filter_classes(\n",
    "            torch.tensor(bboxes_all),\n",
    "            torch.tensor(classes_all),\n",
    "            torch.tensor(scores_all),\n",
    "            classes_to_detect,\n",
    "        )\n",
    "\n",
    "        # if needed, project the class into [0,6] (to match with the annotations in our dataset)\n",
    "        if is_project_class == True:\n",
    "            classes_all = project_class(classes_all)\n",
    "\n",
    "    # record the current time again and calculate the running time\n",
    "    time2 = time.time()\n",
    "    # print(time2 - time1)\n",
    "    \n",
    "    return bboxes_all, classes_all, scores_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Improved Object Detection on a Panoramic Video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To realize the improved object detection on each frame of a video with the functions above, a function called Object_Detection() is defined as below.\n",
    "\n",
    "<b>Parameters of Object_Detection():</b>\n",
    "\n",
    "- <b>input_video_path:</b> Path of the input video;\n",
    "\n",
    "- <b>output_video_path:</b> Path of the output video;\n",
    "\n",
    "- <b>classes_to_detect:</b> Index numbers of the categories to detect in the COCO dataset, [0, 1, 2, 3, 5, 7, 9] by default;\n",
    "\n",
    "- <b>FOV:</b> Field of view of the sub images, 120 by default;\n",
    "\n",
    "- <b>THETAs:</b> A list which contains the theta of each sub image (The length should be the same as the number of sub images),[0, 90, 180, 270] by default;\n",
    "\n",
    "- <b>PHIs:</b> A list which contains the Phi of each sub image (The length should be the same as the number of sub images), [-10, -10, -10, -10] by default;\n",
    "\n",
    "- <b>sub_image_width:</b> Width (or height) of the sub images, 640 by default;\n",
    "\n",
    "- <b>model_type:</b> A string that determines which detector to use (\"YOLO\" or \"Faster RCNN\"), \"YOLO\" by default;\n",
    "\n",
    "- <b>score_threshold:</b> The threshold of the confidence score, 0.4 by default;\n",
    "\n",
    "- <b>nms_threshold:</b> The threshold of the Non Maximum Suppression, 0.45 by default;\n",
    "\n",
    "- <b>use_mymodel:</b> A boolean value which determines whether to use the improved object detection model, if False, instead of being split into 4 parts, the image will be detected as a whole, True by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# function used to realize object detection on a panoramic video\n",
    "def Object_Detection(input_video_path,output_video_path,classes_to_detect=[0, 1, 2, 3, 5, 7, 9],FOV=120,THETAs=[0, 90, 180, 270],PHIs=[-10, -10, -10, -10],sub_image_width=640,model_type=\"YOLO\",score_threshold=0.4,nms_threshold=0.45,use_mymodel=True):\n",
    "        \n",
    "    # load the pretrained detection model\n",
    "    model,cfg=load_model(model_type,sub_image_width,score_threshold,nms_threshold)\n",
    "\n",
    "    # read the input panoramic video (of equirectangular projection)\n",
    "    video_capture = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    # if the input path is not right, warn the user\n",
    "    if (video_capture.isOpened()==False):\n",
    "        print('Can not open the video file.')\n",
    "    # if right, get some info about the video (width, height, frame count and fps)\n",
    "    else:\n",
    "        video_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        video_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        video_frame_count = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        video_fps = int(round(video_capture.get(cv2.CAP_PROP_FPS)))\n",
    "        # fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        outputfile = cv2.VideoWriter(output_video_path, fourcc, video_fps, (video_width, video_height))\n",
    "    \n",
    "    # output the video info\n",
    "    print(\"The input video is \"+str(video_width)+' in width and '+str(video_height)+\" in height.\")\n",
    "\n",
    "    # the number of current frame\n",
    "    num_of_frame=1\n",
    "\n",
    "    # for each image frame in the video\n",
    "    while video_capture.grab():\n",
    "\n",
    "        time1=time.time()\n",
    "\n",
    "        # get the next image frame\n",
    "        _,im= video_capture.retrieve ()\n",
    "        \n",
    "        \n",
    "        # get the predictions on the current frame\n",
    "        bboxes_all, classes_all, scores_all = predict_one_frame(\n",
    "            FOV,\n",
    "            THETAs,\n",
    "            PHIs,\n",
    "            im,\n",
    "            model,\n",
    "            video_width,\n",
    "            video_height,\n",
    "            sub_image_width,\n",
    "            classes_to_detect,\n",
    "            False,\n",
    "            use_mymodel,\n",
    "            model_type,\n",
    "            True\n",
    "        )\n",
    "        \n",
    "        # create an instance of detectron2 so that the output can be visualized\n",
    "        output_new = Instances(\n",
    "            image_size=[video_width, video_height],\n",
    "            pred_boxes=Boxes(torch.tensor(bboxes_all)),\n",
    "            scores=torch.tensor(scores_all),\n",
    "            pred_classes=torch.tensor(classes_all),\n",
    "        )\n",
    "        \n",
    "        # show the current FPS every 5 frames\n",
    "        time2=time.time()\n",
    "        if num_of_frame%5==0:\n",
    "            print(num_of_frame,'/',video_frame_count)\n",
    "            print(str(1/(time2-time1))+' fps')\n",
    "            \n",
    "        num_of_frame+=1\n",
    "\n",
    "        # use `Visualizer` to draw the predictions on the image\n",
    "        v = Visualizer(\n",
    "            im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.0\n",
    "        )\n",
    "        im = v.draw_instance_predictions(output_new.to(\"cpu\"))\n",
    "        outputfile.write(im.get_image()[:, :, ::-1])\n",
    "\n",
    "    # release the input and output videos\n",
    "    video_capture.release()\n",
    "    outputfile.release()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of how to use the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Checkpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n",
      "Reading a file from 'Detectron2 Model Zoo'\n",
      "Using cache found in /Users/guojingwei/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2022-9-20 Python-3.9.13 torch-1.12.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m6 summary: 378 layers, 35704908 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input video is 5376 in width and 2688 in height.\n",
      "5 / 55\n",
      "1.0971085151055975 fps\n",
      "10 / 55\n",
      "1.1391894035360657 fps\n",
      "15 / 55\n",
      "1.1618261675035788 fps\n",
      "20 / 55\n",
      "1.1436455381152102 fps\n",
      "25 / 55\n",
      "1.125331786494368 fps\n"
     ]
    }
   ],
   "source": [
    "Object_Detection('test.mp4','test_object_detection.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Object Tracking on a Panoramic Video using the Improved Object Detection Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To realize object tracking on a panoramic video using the improved object detection model as its detector, a function called Object_Tracking() is defined as below.\n",
    "\n",
    "<b>Parameters of Object_Tracking():</b>\n",
    "\n",
    "- <b>input_video_path:</b> Path of the input video;\n",
    "\n",
    "- <b>output_video_path:</b> Path of the output video;\n",
    "\n",
    "- <b>MOT_text_path:</b> Path of the output txt file which stores all the MOT tracking results;\n",
    "\n",
    "- <b>prevent_different_classes_match:</b> A boolean value which determines whether to use the support for multiple categories in DeepSORT, True by default;\n",
    "\n",
    "- <b>match_across_boundary:</b> A boolean value which determines whether to use the support for boundary continuity in DeepSORT, True by default;\n",
    "\n",
    "- <b>classes_to_detect:</b> Index numbers of the categories to detect in the COCO dataset, [0, 1, 2, 3, 5, 7, 9] by default;\n",
    "\n",
    "- <b>FOV:</b> Field of view of the sub images, 120 by default;\n",
    "\n",
    "- <b>THETAs:</b> A list which contains the theta of each sub image (The length should be the same as the number of sub images),[0, 90, 180, 270] by default;\n",
    "\n",
    "- <b>PHIs:</b> A list which contains the Phi of each sub image (The length should be the same as the number of sub images), [-10, -10, -10, -10] by default;\n",
    "\n",
    "- <b>sub_image_width:</b> Width (or height) of the sub images, 640 by default;\n",
    "\n",
    "- <b>model_type:</b> A string that determines which detector to use (\"YOLO\" or \"Faster RCNN\"), \"YOLO\" by default;\n",
    "\n",
    "- <b>score_threshold:</b> The threshold of the confidence score, 0.4 by default;\n",
    "\n",
    "- <b>nms_threshold:</b> The threshold of the Non Maximum Suppression, 0.45 by default;\n",
    "\n",
    "- <b>use_mymodel:</b> A boolean value which determines whether to use the improved object detection model, if False, instead of being split into 4 parts, the image will be detected as a whole, True by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort.deep_sort import DeepSort\n",
    "from panoramic_detection.draw_output import draw_boxes\n",
    "\n",
    "# function used to realize object tracking on a panoramic video\n",
    "def Object_Tracking(input_video_path,output_video_path,MOT_text_path, prevent_different_classes_match=True,\n",
    "        match_across_boundary=True,classes_to_detect=[0, 1, 2, 3, 5, 7, 9],FOV=120,THETAs=[0, 90, 180, 270],PHIs=[-10, -10, -10, -10],sub_image_width=640,model_type=\"YOLO\",score_threshold=0.4,nms_threshold=0.45,use_mymodel=True):\n",
    "        \n",
    "    # load the pretrained detection model\n",
    "    model,cfg=load_model(model_type,sub_image_width,score_threshold,nms_threshold)\n",
    "\n",
    "    # read the input panoramic video (of equirectangular projection)\n",
    "    video_capture = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    # if the input path is not right, warn the user\n",
    "    if (video_capture.isOpened()==False):\n",
    "        print('Can not open the video file.')\n",
    "    # if right, get some info about the video (width, height, frame count and fps)\n",
    "    else:\n",
    "        video_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        video_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        video_frame_count = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        video_fps = int(round(video_capture.get(cv2.CAP_PROP_FPS)))\n",
    "        # fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        outputfile = cv2.VideoWriter(output_video_path, fourcc, video_fps, (video_width, video_height))\n",
    "    \n",
    "    # output the video info\n",
    "    print(\"The input video is \"+str(video_width)+' in width and '+str(video_height)+\" in height.\")\n",
    "\n",
    "    # create a deepsort instance with the pre-trained feature extraction model\n",
    "    deepsort = DeepSort('./deep_sort/deep/checkpoint/ckpt.t7', use_cuda=torch.cuda.is_available())\n",
    "\n",
    "    # the number of current frame\n",
    "    num_of_frame=1\n",
    "\n",
    "    with open(MOT_text_path,\"w\") as f: \n",
    "        # for each image frame in the video\n",
    "        while video_capture.grab():\n",
    "\n",
    "            time1=time.time()\n",
    "\n",
    "            # get the next image frame\n",
    "            _,im= video_capture.retrieve ()\n",
    "            \n",
    "            # get the predictions on the current frame\n",
    "            bboxes_all, classes_all, scores_all = predict_one_frame(\n",
    "                FOV,\n",
    "                THETAs,\n",
    "                PHIs,\n",
    "                im,\n",
    "                model,\n",
    "                video_width,\n",
    "                video_height,\n",
    "                sub_image_width,\n",
    "                classes_to_detect,\n",
    "                False,\n",
    "                use_mymodel,\n",
    "                model_type,\n",
    "                not match_across_boundary,\n",
    "            )\n",
    "            \n",
    "            # convert the bboxes from [x,y,x,y] to [xc,yc,w,h]\n",
    "            bboxes_all_xcycwh=xyxy2xcycwh(bboxes_all)\n",
    "            \n",
    "            # update deepsort and get the tracking results\n",
    "            track_outputs = deepsort.update(np.array(bboxes_all_xcycwh),np.array(classes_all),  np.array(scores_all),im, prevent_different_classes_match, match_across_boundary)\n",
    "            \n",
    "            # plot the results on the video and save them as MOT texts\n",
    "            if len(track_outputs) > 0:\n",
    "                bbox_xyxy = track_outputs[:, :4]\n",
    "                track_classes= track_outputs[:, 4]\n",
    "                track_scores= track_outputs[:, 5]\n",
    "                identities = track_outputs[:, -1]\n",
    "                im = draw_boxes(im, bbox_xyxy, track_classes, track_scores, video_width,identities)\n",
    "                for bb_xyxy,track_class,identity in zip(bbox_xyxy,track_classes,identities):\n",
    "                        f.write(str(num_of_frame)+','+str(int(identity))+','+str(deepsort._xyxy_to_tlwh(bb_xyxy)).strip('(').strip(')').replace(' ','')+','+'-1,-1,-1,-1\\n')\n",
    "            outputfile.write(im)\n",
    "\n",
    "            # show the current FPS\n",
    "            time2=time.time()\n",
    "            if num_of_frame%5==0:\n",
    "                print(num_of_frame,'/',video_frame_count)\n",
    "                print(str(1/(time2-time1))+' fps')\n",
    "                \n",
    "            num_of_frame+=1\n",
    "\n",
    "    # release the input and output videos\n",
    "    video_capture.release()\n",
    "    outputfile.release()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of how to use the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Checkpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n",
      "Reading a file from 'Detectron2 Model Zoo'\n",
      "Using cache found in /Users/guojingwei/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2022-9-20 Python-3.9.13 torch-1.12.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m6 summary: 378 layers, 35704908 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "Loading weights from ./deep_sort/deep/checkpoint/ckpt.t7... Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input video is 5376 in width and 2688 in height.\n",
      "10 / 55\n",
      "0.7008337044163192 fps\n",
      "20 / 55\n",
      "0.8350278551393394 fps\n"
     ]
    }
   ],
   "source": [
    "Object_Tracking('test.mp4','test_tracking.mp4','test_tracking.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Overtaking Behaviour Detection on a Panoramic Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To realize overtaking behaviour detection on a panoramic video using DeepSORT, a function called Overtaking_Detection() is defined as below.\n",
    "\n",
    "<b>Parameters of Overtaking_Detection():</b>\n",
    "\n",
    "- <b>input_video_path:</b> Path of the input video;\n",
    "\n",
    "- <b>output_video_path:</b> Path of the output video;\n",
    "\n",
    "- <b>mode:</b> A string that determines which kind of overtaking behaviour to detect, \"Confirmed\" or \"Unconfirmed\", \"Confirmed\" by default;\n",
    "\n",
    "- <b>prevent_different_classes_match:</b> A boolean value which determines whether to use the support for multiple categories in DeepSORT, True by default;\n",
    "\n",
    "- <b>match_across_boundary:</b> A boolean value which determines whether to use the support for boundary continuity in DeepSORT, True by default;\n",
    "\n",
    "- <b>classes_to_detect:</b> Index numbers of the categories to detect in the COCO dataset, [0, 1, 2, 3, 5, 7, 9] by default;\n",
    "\n",
    "- <b>classes_to_detect_movement:</b> Index numbers of the categories for movement detection in the COCO dataset, which should be a subset of classes_to_detect, [2,5,7] (i.e., car,bus and truck) by default;\n",
    "\n",
    "- <b>size_thresholds:</b> A set of size thresholds which should share the same length with classes_to_detect_movement, if the size of a track of a certain class is larger than the corresponding threshold, then it is considered as close to the user, [500 * 500, 900 * 900, 600 * 600] by default;\n",
    "\n",
    "- <b>FOV:</b> Field of view of the sub images, 120 by default;\n",
    "\n",
    "- <b>THETAs:</b> A list which contains the theta of each sub image (The length should be the same as the number of sub images),[0, 90, 180, 270] by default;\n",
    "\n",
    "- <b>PHIs:</b> A list which contains the Phi of each sub image (The length should be the same as the number of sub images), [-10, -10, -10, -10] by default;\n",
    "\n",
    "- <b>sub_image_width:</b> Width (or height) of the sub images, 640 by default;\n",
    "\n",
    "- <b>model_type:</b> A string that determines which detector to use (\"YOLO\" or \"Faster RCNN\"), \"YOLO\" by default;\n",
    "\n",
    "- <b>score_threshold:</b> The threshold of the confidence score, 0.4 by default;\n",
    "\n",
    "- <b>nms_threshold:</b> The threshold of the Non Maximum Suppression, 0.45 by default;\n",
    "\n",
    "- <b>use_mymodel:</b> A boolean value which determines whether to use the improved object detection model, if False, instead of being split into 4 parts, the image will be detected as a whole, True by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort.deep_sort import DeepSort\n",
    "from panoramic_detection.draw_output import draw_boxes\n",
    "\n",
    "# a function used to realize overtaking behaviour detection on a panoramic video\n",
    "def Overtaking_Detection(input_video_path,output_video_path, mode='Confirmed',prevent_different_classes_match=True,\n",
    "        match_across_boundary=True,classes_to_detect=[0, 1, 2, 3, 5, 7, 9],classes_to_detect_movement=[2,5,7],size_thresholds=[500 * 500, 900 * 900, 600 * 600],FOV=120,THETAs=[0, 90, 180, 270],PHIs=[-10, -10, -10, -10],sub_image_width=640,model_type=\"YOLO\",score_threshold=0.4,nms_threshold=0.45,use_mymodel=True):\n",
    "        \n",
    "    # load the pretrained detection model\n",
    "    model,cfg=load_model(model_type,sub_image_width,score_threshold,nms_threshold)\n",
    "\n",
    "    # read the input panoramic video (of equirectangular projection)\n",
    "    video_capture = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    # if the input path is not right, warn the user\n",
    "    if (video_capture.isOpened()==False):\n",
    "        print('Can not open the video file.')\n",
    "    # if right, get some info about the video (width, height, frame count and fps)\n",
    "    else:\n",
    "        video_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        video_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        video_frame_count = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        video_fps = int(round(video_capture.get(cv2.CAP_PROP_FPS)))\n",
    "        # fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        outputfile = cv2.VideoWriter(output_video_path, fourcc, video_fps, (video_width, video_height))\n",
    "    \n",
    "    # output the video info\n",
    "    print(\"The input video is \"+str(video_width)+' in width and '+str(video_height)+\" in height.\")\n",
    "\n",
    "    # create a deepsort instance with the pre-trained feature extraction model\n",
    "    deepsort = DeepSort('./deep_sort/deep/checkpoint/ckpt.t7', use_cuda=torch.cuda.is_available())\n",
    "\n",
    "    # the number of current frame\n",
    "    num_of_frame=1\n",
    "\n",
    "    # a dictionary which stores the history positions of each track\n",
    "    history_track_positions={}\n",
    "\n",
    "    # dics/lists to store the unconfirmed left/right overtaking, confirmed overtaking and their periods\n",
    "    unconfirmed_left_overtaking={}\n",
    "    unconfirmed_right_overtaking={}\n",
    "    confirmed_overtaking=[]\n",
    "    confirmed_overtaking_period=[]\n",
    "\n",
    "    with open('results.txt',\"w\") as f: \n",
    "        \n",
    "        # for each image frame in the video\n",
    "        while video_capture.grab():\n",
    "\n",
    "            time1=time.time()\n",
    "\n",
    "            # get the next image frame\n",
    "            _,im= video_capture.retrieve ()\n",
    "            \n",
    "            # get the predictions on the current frame\n",
    "            bboxes_all, classes_all, scores_all = predict_one_frame(\n",
    "                FOV,\n",
    "                THETAs,\n",
    "                PHIs,\n",
    "                im,\n",
    "                model,\n",
    "                video_width,\n",
    "                video_height,\n",
    "                sub_image_width,\n",
    "                classes_to_detect,\n",
    "                False,\n",
    "                use_mymodel,\n",
    "                model_type,\n",
    "                not match_across_boundary,\n",
    "            )\n",
    "            \n",
    "            # convert the bboxes from [x,y,x,y] to [xc,yc,w,h]\n",
    "            bboxes_all_xcycwh=xyxy2xcycwh(bboxes_all)\n",
    "            \n",
    "            # update deepsort and get the tracking results\n",
    "            track_outputs = deepsort.update(np.array(bboxes_all_xcycwh),np.array(classes_all),  np.array(scores_all),im, prevent_different_classes_match, match_across_boundary)\n",
    "            \n",
    "            # two lists to store the objects that are moving forwards and backwards\n",
    "            objects_moving_forwards=[]\n",
    "            objects_moving_backwards=[]\n",
    "\n",
    "            # if there are tracked objects in the current frame\n",
    "            if len(track_outputs) > 0:\n",
    "                bbox_xyxy = track_outputs[:, :4]\n",
    "                track_classes= track_outputs[:, 4]\n",
    "                track_scores= track_outputs[:, 5]\n",
    "                identities = track_outputs[:, -1]\n",
    "                \n",
    "                # for each track\n",
    "                for bb_xyxy,track_class,identity in zip(bbox_xyxy,track_classes,identities):\n",
    "                    \n",
    "                    # save the tracking results to the txt file\n",
    "                    f.write(str(num_of_frame)+','+str(int(identity))+','+str(deepsort._xyxy_to_tlwh(bb_xyxy)).strip('(').strip(')').replace(' ','')+','+'-1,-1,-1,-1\\n')\n",
    "                    \n",
    "                    # check whether the track is moving forwards or backwards\n",
    "                    bb_xyxy_list=bb_xyxy.tolist()\n",
    "                    \n",
    "                    # if the track is doing an unconfirmed overtake from the left of the image\n",
    "                    if int(identity) in unconfirmed_left_overtaking:\n",
    "                        # if the rear of the track has passed the 90 degree, update the overtake to confirmed\n",
    "                        if bb_xyxy_list[0]>=video_width/360*90:\n",
    "                            confirmed_overtaking.append(int(identity))\n",
    "                            confirmed_overtaking_period.append([unconfirmed_left_overtaking[int(identity)],num_of_frame])\n",
    "                            unconfirmed_left_overtaking.pop(int(identity))\n",
    "                        # if the front of the track has come back, delete the unconfirmed overtake\n",
    "                        elif bb_xyxy_list[2]<video_width/360*90:\n",
    "                            unconfirmed_left_overtaking.pop(int(identity))\n",
    "                    \n",
    "                    # if the track is doing an unconfirmed overtake from the right of the image\n",
    "                    elif int(identity) in unconfirmed_right_overtaking:\n",
    "                        # if the rear of the track has passed the 270 degree, update the overtake to confirmed\n",
    "                        if bb_xyxy_list[2]<=video_width/360*270:\n",
    "                            confirmed_overtaking.append(int(identity))\n",
    "                            confirmed_overtaking_period.append([unconfirmed_right_overtaking[int(identity)],num_of_frame])\n",
    "                            unconfirmed_right_overtaking.pop(int(identity))\n",
    "                        # if the front of the track has come back, delete the unconfirmed overtake\n",
    "                        elif bb_xyxy_list[0]>video_width/360*270:\n",
    "                            unconfirmed_right_overtaking.pop(int(identity))\n",
    "                    \n",
    "                    # if the track is not doing an overtake and its class is on which we need to detect overtakes\n",
    "                    if track_class in classes_to_detect_movement:\n",
    "                        # add the current position of the track to a dictionary called history_track_positions\n",
    "                        if int(identity) not in history_track_positions.keys():\n",
    "                            history_track_positions[int(identity)]= [bb_xyxy_list]\n",
    "                        else:\n",
    "                            history_track_positions[int(identity)]+= [bb_xyxy_list]\n",
    "                            # count how many times a track moves forwards and backwards in the last five frames\n",
    "                            if len(history_track_positions[int(identity)])>=6:\n",
    "                                forwards_num=0\n",
    "                                backwards_num=0\n",
    "                                for ii in range (-6,-1):\n",
    "                                    if abs(video_width/2-xyxy2xcycwh(history_track_positions[int(identity)])[ii][0])>abs(video_width/2-xyxy2xcycwh(history_track_positions[int(identity)])[ii+1][0]):\n",
    "                                        forwards_num+=1\n",
    "                                    else: backwards_num+=1\n",
    "                                # if in the last 5 frames, at least 3 frames moves towards the middle line of the image\n",
    "                                if forwards_num>=3:\n",
    "                                    # treat the object as it is moving forwards\n",
    "                                    objects_moving_forwards.append(int(identity))\n",
    "                                    # if in the last frame, the front of the track had not passed the 90/270 degree line, but now it has\n",
    "                                    # give the track an unconfirmed overtaking behaviour\n",
    "                                    if bb_xyxy_list[2]>=video_width/360*90 and history_track_positions[int(identity)][-2][2]<video_width/360*90:\n",
    "                                        unconfirmed_left_overtaking[int(identity)]=num_of_frame\n",
    "                                    elif bb_xyxy_list[0]<=video_width/360*270 and history_track_positions[int(identity)][-2][0]>video_width/360*270:\n",
    "                                        unconfirmed_right_overtaking[int(identity)]=num_of_frame\n",
    "                                # if in the last 5 frames, at least 3 frames moves away from the middle line of the image\n",
    "                                elif backwards_num>=3:\n",
    "                                    # treat the object as it is moving backwards\n",
    "                                    objects_moving_backwards.append(int(identity))\n",
    "            \n",
    "                # if the function is used for unconfirmed overtaking behaviour detection, draw the tracks with the overtaking boxes\n",
    "                if mode=='Unconfirmed':\n",
    "                    im = draw_boxes(im, bbox_xyxy, track_classes, track_scores,video_width, identities,objects_moving_backwards,objects_moving_forwards,unconfirmed_left_overtaking,unconfirmed_right_overtaking,size_thresholds,True,classes_to_detect_movement)\n",
    "                \n",
    "                # if the function is used for confirmed overtaking behaviour detection, only draw the tracks\n",
    "                elif mode=='Confirmed':\n",
    "                    im = draw_boxes(im, bbox_xyxy, track_classes, track_scores,video_width, identities,objects_moving_backwards,objects_moving_forwards)\n",
    "\n",
    "            # save the frame to the output file\n",
    "            outputfile.write(im)\n",
    "\n",
    "            # show the current FPS\n",
    "            time2=time.time()\n",
    "            if num_of_frame%5==0:\n",
    "                print(num_of_frame,'/',video_frame_count)\n",
    "                print(str(1/(time2-time1))+' fps')\n",
    "                \n",
    "            num_of_frame+=1\n",
    "\n",
    "    # release the input and output videos\n",
    "    video_capture.release()\n",
    "    outputfile.release()\n",
    "    \n",
    "    # since the confirmed overtakes can only be detected after the whole behaviour has been finished\n",
    "    # in the 'Confirmed' mode, draw the boxes for comfirmed overtakes after the process of detection\n",
    "    if  mode=='Confirmed':\n",
    "\n",
    "        print('Confirmed overtaking tracks:',confirmed_overtaking)\n",
    "        print('Confirmed overtaking periods:', confirmed_overtaking_period)\n",
    "\n",
    "        # copy and paste the output video with tracking results\n",
    "        v_src = open(output_video_path,'rb')\n",
    "        content = v_src.read()\n",
    "        v_copy = open('copy.mp4','wb')\n",
    "        v_copy.write(content)\n",
    "        v_src.close()\n",
    "        v_copy.close()\n",
    "\n",
    "        video_capture = cv2.VideoCapture('copy.mp4')\n",
    "        video_frame_count = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        outputfile = cv2.VideoWriter(output_video_path, fourcc, video_fps, (video_width, video_height))\n",
    "        num_of_frame = 1 \n",
    "        \n",
    "        tracking_results=[]\n",
    "\n",
    "        # read the tracking results\n",
    "        with open('results.txt',\"r\") as f:   \n",
    "            data=f.readlines()\n",
    "            for line in data:\n",
    "                tracking_results.append(line)\n",
    "        \n",
    "        # for each image frame in the video\n",
    "        while video_capture.grab():\n",
    "            # get the next image frame\n",
    "            _,im= video_capture.retrieve ()\n",
    "            \n",
    "            \n",
    "            # when an track is between the start and end frames of an confirmed overtaking behaviour\n",
    "            for i,q in zip(confirmed_overtaking_period,confirmed_overtaking):\n",
    "                # color the bbox of the track with red\n",
    "                if num_of_frame in range(i[0],i[1]):\n",
    "                    for line in tracking_results:\n",
    "                        contents=line.split(',')\n",
    "                        if contents[0]==str(num_of_frame) and contents[1]==str(q):\n",
    "                            red_area = np.zeros(im.shape, np.uint8)\n",
    "                            cv2.rectangle(red_area, (int(float(contents[2])),int(float(contents[3]))), (int(float(contents[2]))+int(float(contents[4])),int(float(contents[3]))+int(float(contents[5]))), (0, 0, 255), -1)\n",
    "                            im = cv2.addWeighted(im, 1.0, red_area, 0.5, 1)\n",
    "            outputfile.write(im)\n",
    "            if num_of_frame%5==0:\n",
    "                print(num_of_frame,'/',video_frame_count)\n",
    "            num_of_frame+=1\n",
    "        \n",
    "        # release the videos again\n",
    "        video_capture.release()\n",
    "        outputfile.release()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of how to use the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Checkpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n",
      "Reading a file from 'Detectron2 Model Zoo'\n",
      "Using cache found in /Users/guojingwei/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2022-9-20 Python-3.9.13 torch-1.12.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m6 summary: 378 layers, 35704908 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "Loading weights from ./deep_sort/deep/checkpoint/ckpt.t7... Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input video is 5376 in width and 2688 in height.\n",
      "5 / 55\n",
      "0.5649516674025502 fps\n",
      "10 / 55\n",
      "0.5448937709541225 fps\n",
      "15 / 55\n",
      "0.49607409584511875 fps\n",
      "20 / 55\n",
      "0.5224900464389478 fps\n",
      "25 / 55\n",
      "0.4886474591882539 fps\n"
     ]
    }
   ],
   "source": [
    "Overtaking_Detection('test.mp4','test_overtake.mp4',mode='Unconfirmed',classes_to_detect_movement=[2,5,7],size_thresholds=[500 * 500, 900 * 900, 600 * 600])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('detectron_m1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d324e075bdfc27a4b7f2070b5988b56251856b7a41955a66bd7a6c0536bdbce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
